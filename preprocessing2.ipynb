{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1665166548545,
     "user": {
      "displayName": "Aniket Yadav",
      "userId": "11965919448312311802"
     },
     "user_tz": -330
    },
    "id": "CzZF-CbKutSA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "error",
     "timestamp": 1665166763184,
     "user": {
      "displayName": "Aniket Yadav",
      "userId": "11965919448312311802"
     },
     "user_tz": -330
    },
    "id": "6pjZL0KjvakS",
    "outputId": "dc9c2424-c3ef-42a1-87d8-0a88fc3c2fcd"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sentence import Sentence\n",
    "\n",
    "nltk.download('punkt')\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "class Preprocessing(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # def processFilev2(self, f):\n",
    "    #         text_0 = f\n",
    "\n",
    "    #         tokenizer_sent = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    #         seperate_line = tokenizer_sent.tokenize(text_0.strip())\n",
    "\n",
    "    #         sentences = []\n",
    "\n",
    "    #         for sent in seperate_line:\n",
    "    #             sent = sent.strip()\n",
    "    #             Original_sent = sent[:]\n",
    "    #             sent = sent.lower()\n",
    "    #             line = nltk.word_tokenize(sent)\n",
    "\n",
    "    #             cleaned_sent = [porter.stem(word) for word in line]\n",
    "    #             cleaned_sent = list(filter(lambda x: x != '.' and x != '`' and x != ',' and x != '_' and x != ';'\n",
    "    #                                                         and x != '(' and x != ')' and x.find('&') == -1\n",
    "    #                                                         and x != '?' and x != \"'\" and x != '!' and x != '''\"'''\n",
    "    #                                                         and x != '``' and x != '--' and x != ':'\n",
    "    #                                                         and x != \"''\" and x != \"'s\", cleaned_sent))\n",
    "\n",
    "    #             if (len(cleaned_sent) <= 4):\n",
    "    #                 continue\n",
    "\n",
    "    #             if cleaned_sent:\n",
    "    #                 sentences.append(Sentence(f, cleaned_sent, Original_sent))\n",
    "\n",
    "    #         return sentences\n",
    "\n",
    "    def process_the_File(self, f):\n",
    "\n",
    "            text_0 = f\n",
    "            \n",
    "            text_0 = re.sub(\"\\n\", \" \", text_0)\n",
    "            text_1 = re.sub(\"\\\"\", \"\\\"\", text_0)\n",
    "            text_1 = re.sub(\"''\", \"\\\"\", text_1)\n",
    "            text_1 = re.sub(\"``\", \"\\\"\", text_1)\n",
    "            text_1 = re.sub(\" +\", \" \", text_1)\n",
    "            text_1 = re.sub(\" _ \", \"\", text_1)\n",
    "\n",
    "            text_1 = re.sub(\"&\\w+;\", \" \", text_1)\n",
    "\n",
    "            tokenizer_sent = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "            seperate_line = tokenizer_sent.tokenize(text_0.strip())\n",
    "\n",
    "            sentences = []\n",
    "\n",
    "            for sent in seperate_line:\n",
    "                sent = sent.strip()\n",
    "                Original_sent = sent[:]\n",
    "                sent = sent.lower()\n",
    "                line = nltk.word_tokenize(sent)\n",
    "\n",
    "                cleaned_sent = [porter.stem(word) for word in line]\n",
    "                cleaned_sent = list(filter(lambda x: x != '.' and x != '`' and x != ',' and x != '_' and x != ';'\n",
    "                                                            and x != '(' and x != ')' and x.find('&') == -1\n",
    "                                                            and x != '?' and x != \"'\" and x != '!' and x != '''\"'''\n",
    "                                                            and x != '``' and x != '--' and x != ':'\n",
    "                                                            and x != \"''\" and x != \"'s\", cleaned_sent))\n",
    "\n",
    "                if (len(cleaned_sent) <= 4):\n",
    "                    continue\n",
    "\n",
    "                if cleaned_sent:\n",
    "                    sentences.append(Sentence(f, cleaned_sent, Original_sent))\n",
    "\n",
    "            return sentences\n",
    "\n",
    "    def directory_opener(self, folder, mode=\"train\"):\n",
    "        sentences = []\n",
    "        last_indexs = []\n",
    "        doc=folder[\"document\"]\n",
    "        files=doc.split(\"|||||\")\n",
    "#         print(files)\n",
    "        for f in files:\n",
    "#             print(f)\n",
    "            last_indexs.append(len(sentences))\n",
    "            if mode == \"train\":\n",
    "                sentences = sentences + self.process_the_File(f)\n",
    "            # else:\n",
    "            #     sentences = sentences + self.process_the_Filev2(f)\n",
    "\n",
    "        return sentences, last_indexs\n",
    "\n",
    "        \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "executionInfo": {
     "elapsed": 11952,
     "status": "error",
     "timestamp": 1665166602995,
     "user": {
      "displayName": "Aniket Yadav",
      "userId": "11965919448312311802"
     },
     "user_tz": -330
    },
    "id": "LWNJOnaXlyvk",
    "outputId": "9f1bd5cb-b9df-4032-b903-188978e20cd6"
   },
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7_0k9a1qb_Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNO+jQ+zdECE4DgOgn7r/D/",
   "mount_file_id": "1lnrZ1jB-6QUBMTUMg3s9bd1_oUe3FYy7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
